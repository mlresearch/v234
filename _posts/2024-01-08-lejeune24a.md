---
title: An Adaptive Tangent Feature Perspective of Neural Networks
openreview: NFqCYfoLw6
abstract: In order to better understand feature learning in neural networks, we propose
  and study linear models in tangent feature space where the features are allowed
  to be transformed during training. We consider linear feature transformations, resulting
  in a joint optimization over parameters and transformations with a bilinear interpolation
  constraint. We show that this relaxed optimization problem has an equivalent linearly
  constrained optimization with structured regularization that encourages approximately
  low rank solutions. Specializing to structures arising in neural networks, we gain
  insights into how the features and thus the kernel function change, providing additional
  nuance to the phenomenon of kernel alignment when the target function is poorly
  represented by tangent features. We verify our theoretical observations in the kernel
  alignment of real neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lejeune24a
month: 0
tex_title: An Adaptive Tangent Feature Perspective of Neural Networks
firstpage: 379
lastpage: 394
page: 379-394
order: 379
cycles: false
bibtex_author: LeJeune, Daniel and Alemohammad, Sina
author:
- given: Daniel
  family: LeJeune
- given: Sina
  family: Alemohammad
date: 2024-01-08
address:
container-title: Conference on Parsimony and Learning
volume: '234'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 8
pdf: https://proceedings.mlr.press/v234/lejeune24a/lejeune24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
